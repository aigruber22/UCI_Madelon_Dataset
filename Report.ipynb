{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement ##\n",
    "\n",
    "Predicting the classes of a synthetic data set is challenging due to lack of feature context. The goal of this endeavor was to use unsupervised and supervised approaches to identify important features in the data set and predict a binary class.\n",
    "\n",
    "## The Data ##\n",
    "\n",
    "**UCI's Madelon:** This data set was provide in 5 separate chunks. There was a data and labels pairing for both 'training' and 'validation' data. The fifth set was a 'test' set with no labels. I combined the training and validation data so I could conduct my own cross validation. I did not use the test data. The resulting data set:\n",
    "\n",
    "2000 rows, 501 columns\n",
    "\n",
    "It is provided that this data set has 20 important features, and 480 noisey features. The classes (1 or -1) exist in 16 clusters per class in a hypercube, clustering at different corners of said hypercube.\n",
    "    \n",
    "**Josh Cook Data:** This data set was stored on a SQL server. Unlike the Madelon, all the data was in one database.\n",
    "\n",
    "200,000 rows, 1,001 columns\n",
    "\n",
    "It is provided that this data set has some number of important features, where the rest are noisey features. The classes (1 or 0) exist in 16 clusters per class in a hypercube, clustering at different corners of said hypercube.\n",
    "\n",
    "## Overall Approach ##\n",
    "\n",
    "I first conducted my analysis on a subset of each data set. I took a 10% sample of UCI's Madelon data and a 3.3% sample of the Josh Cook data. I then took my feature selection and modeling approaches and attempted to apply them to each full data set. Subsequently, the report is split into four sections: UCI's Madelon Subset, UCI's Madelon Full Set, Josh Cook Data Subset, and Josh Cook Data Full Set.\n",
    "\n",
    "The following steps were taken for each of those four sections:\n",
    "\n",
    "**Benchmarking **\n",
    "\n",
    "In order to establish a baseline to measure the models against, I conducted 4 naive tests: Logistic Regression, Decision Tree Classifier, K Nearest Neighbor, and Support Vector Classifier.\n",
    "\n",
    "When applicable, I kept regularization strength low and used predominately default hyperparameters. No feature selection, preprocessing, or hyperparameter tuning was used in this step.\n",
    "\n",
    "Note: I did not conduct a separate benchmarking analysis for a subset of the UCI Madelon data. The report below will show identical results for UCI Subset and UCI Full Set only on the benchmarking section.\n",
    "\n",
    "**Feature Selection**\n",
    "\n",
    "Given what I know about the data and the classes, I first performed unsupervised approaches to try and determine important features. Since I know that there are only 20 important features out of 500, I thought I could check to see if there were correlations across features. By using my ```.corr()``` method, I was able to discern 20 features with significant correlation to at least 1 other feature. I've provided the section of code used to do this:\n",
    "\n",
    "```predictors_corr = predictors.corr()\n",
    "hi_corrs = predictors_corr.abs() > .5\n",
    "hi_count = predictors_corr[hi_corrs].count() > 1\n",
    "top_corrs = list(predictors_corr[hi_count].index)```\n",
    "\n",
    "Essentially, this is measuring the correlation of each feature with every other feature in the data set (including itself). The code above is checking to look for correlations greater than .5. If a feature is correlated with more than 1 feature at that threshold (because every feature will have a correlation of 1.0 with itself), it gets flagged as being a top correlated feature. This resulted in 20 important features in both UCI's Madelon data set and the Josh Cook data set.\n",
    "\n",
    "I also applied several supervised feature selection approaches, but given the dimensionality of the data, they did not provide consistent results across them. I decided to go with the 20 features achieved in the correlation analysis.\n",
    "\n",
    "**Modeling**\n",
    "\n",
    "For the modeling, I used the same four models that were used in the benchmarking. However, this time around I included only my 20 important features. Additionally, I applied preprocessing and hyperparameter tuning to the model. Each section in the extended report lists those steps in building my models. \n",
    "\n",
    "What I discovered was that **K Nearest Neighbors** best predicted my classes. Given that we know the data exists in a multi-dimensional hypercube (as explained in the data section above), it makes sense that this would be the strongest approach. The reason is that K Nearest Neighbors assigns class based on distance between data points.\n",
    "\n",
    "On the full UCI Madelon data, I was able to predict at a rate of 89% accuracy the classes for each record. On the full Josh Cook data, I was able to predict at a rate of 76% accuracy.\n",
    "\n",
    "## Next Steps ##\n",
    "\n",
    "To improve prediction accuracy, there are two things that I'd do next.\n",
    "\n",
    "1. Stratify y in my train test split in order to ensure class balance: This makes sure that when I build my training and test data sets, I help to mitigate the risk of class imbalance.\n",
    "\n",
    "2. Further tune hyperparameter in my K Nearest Neighbor: I want to test a larger range of ```n_neighbors```.\n",
    "\n",
    "## Extended Report Overview ##\n",
    "\n",
    "Below I've included details on the steps taken in each of the four sections I mentioned above. Those four sections are organized below in the following order:\n",
    "\n",
    "1. UCI's Madelon Subset\n",
    "2. UCI's Madelon Full Set\n",
    "3. Josh Cook's Subset\n",
    "4. Josh Cook's Full Set\n",
    "\n",
    "In each of those four sections, you'll see details on the benchmarking, feature selection, and modeling steps taken in the Jupyter notebooks that pertain to each section and stored in this repository.\n",
    "\n",
    "_________\n",
    "\n",
    "# Extended Report #\n",
    "\n",
    "## UCI's Madelon Data Set - Subset ##\n",
    "\n",
    "### Benchmarking ###\n",
    "\n",
    "***Description of Approach:***\n",
    "\n",
    "Built naive models to get a prediction benchmark for our classification problem. The models used are:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier\n",
    "3. K Nearest Neighbors\n",
    "4. Support Vector Classifier\n",
    "\n",
    "Decision Tree Classifier performed best with ROC AUC Score of .754 and Log Loss of 8.502.\n",
    "\n",
    "### Benchmarking Scores\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "*ROC AUC Score:* 0.520\n",
    "\n",
    "*Log Loss:* 16.605\n",
    "\n",
    "**Decision Tree Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.754\n",
    "\n",
    "*Log Loss:* 8.502\n",
    "\n",
    "**K Nearest Neighbors:**\n",
    "\n",
    "*ROC AUC Score:* 0.696\n",
    "\n",
    "*Log Loss:* 10.495\n",
    "\n",
    "**Support Vector Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.500\n",
    "\n",
    "*Log Loss:* 17.469\n",
    "\n",
    "### Feature Selection ###\n",
    "\n",
    "*** Description of Approach: ***\n",
    "\n",
    "In this approach, I conduct several methods of feature selection. These methods include both supervised and unsupervised approaches.\n",
    "\n",
    "Contents:\n",
    "1. Data Import\n",
    "2. Unsupervised Approach\n",
    "    - Looked at correlation (```.corr()```) between features and returned those features that are highly correlated ( > 0.5) with at least one other feature.\n",
    "    - The result was 20 features: \n",
    "        - 28, 48, 64, 105, 128, 153, 241, 281, 318, 336, 338, 378, 433, 442, 451, 453, 455, 472, 475, 493\n",
    "    - Visualizations of those 20 features\n",
    "3. Supervised Approaches\n",
    "    - Select from Model w/ Logistic Regression (L2 penalty)\n",
    "    - Select from Model w/ Logistic Regression (L1 penalty)\n",
    "    - SelectKBest w/ k = 20\n",
    "    - RFE w/ RandomForestClassifier as estimator\n",
    "\n",
    "These approaches all gave me various feature importances and coefficients with little consistency across them.\n",
    "    \n",
    "**Results:** I decided to use the 20 features gained from the correlation approach. Given that I know the madelon data has 20 important (or redundant) features and 480 noise, I felt comfortable with the approach that produced exactly 20 features.\n",
    "\n",
    "### Modeling ###\n",
    "\n",
    "***Description of Approach:***\n",
    "\n",
    "In alignment with the benchmarking approach (for this workflow, the benchmarking was done on the full data set and not the sample used for modeling), I modeled the sample on these four models: Logistic Regression, Decision Tree, K Nearest Neighbors, and Support Vector Classifier.\n",
    "\n",
    "The full steps of modeling:\n",
    "1. Train Test Split\n",
    "2. Min Max Scaler\n",
    "3. Deskewing (Boxcox)\n",
    "4. PCA (5 components)\n",
    "5. Standard Scaler\n",
    "6. Model\n",
    "\n",
    "Steps 5 and 6 were built into a pipeline and gridsearched on to tune hyperparameters.\n",
    "\n",
    "The best performing model was K Nearest Neighbors with an ROC AUC Score of .740 and Log Loss of 8.635.\n",
    "\n",
    "### Modeling Scores ###\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "*ROC AUC Score:* 0.501\n",
    "\n",
    "*Log Loss:* 16.605\n",
    "\n",
    "**Decision Tree:**\n",
    "\n",
    "*ROC AUC Score:* 0.618\n",
    "\n",
    "*Log Loss:* 12.620\n",
    "\n",
    "**K Nearest Neighbors:**\n",
    "\n",
    "*ROC AUC Score:* 0.740\n",
    "\n",
    "*Log Loss:* 8.635\n",
    "\n",
    "**Support Vector Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.458\n",
    "\n",
    "*Log Loss:* 17.934\n",
    "\n",
    "_________\n",
    "\n",
    "## UCI's Madelon Data Set - Full Set ##\n",
    "\n",
    "### Benchmarking ###\n",
    "\n",
    "***Description of Approach:***\n",
    "\n",
    "Built naive models to get a prediction benchmark for our classification problem. The models used are:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier\n",
    "3. K Nearest Neighbors\n",
    "4. Support Vector Classifier\n",
    "\n",
    "Decision Tree Classifier performed best with ROC AUC Score of .754 and Log Loss of 8.502.\n",
    "\n",
    "### Benchmarking Scores\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "*ROC AUC Score:* 0.520\n",
    "\n",
    "*Log Loss:* 16.605\n",
    "\n",
    "**Decision Tree Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.754\n",
    "\n",
    "*Log Loss:* 8.502\n",
    "\n",
    "**K Nearest Neighbors:**\n",
    "\n",
    "*ROC AUC Score:* 0.696\n",
    "\n",
    "*Log Loss:* 10.495\n",
    "\n",
    "**Support Vector Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.500\n",
    "\n",
    "*Log Loss:* 17.469\n",
    "\n",
    "### Feature Selection ###\n",
    "\n",
    "*** Description of Approach: ***\n",
    "\n",
    "In this approach, I conduct several methods of feature selection. These methods include both supervised and unsupervised approaches. I've run these methods on the full UCI Madelon data set.\n",
    "\n",
    "Contents:\n",
    "1. Data Import\n",
    "2. Unsupervised Approach\n",
    "    - Looked at correlation (```.corr()```) between features and returned those features that are highly correlated ( > 0.5) with at least one other feature.\n",
    "    - The result was 20 features: \n",
    "        - 28, 48, 64, 105, 128, 153, 241, 281, 318, 336, 338, 378, 433, 442, 451, 453, 455, 472, 475, 493\n",
    "    - Visualizations of those 20 features\n",
    "3. Supervised Approaches\n",
    "    - Select from Model w/ Logistic Regression (L2 penalty)\n",
    "    - Select from Model w/ Logistic Regression (L1 penalty)\n",
    "    - SelectKBest w/ k = 20\n",
    "    - RFE w/ RandomForestClassifier as estimator\n",
    "    - These approaches all gave me various feature importances and coefficients with little consistency across them.\n",
    "    \n",
    "**Results:** I decided to use the 20 features gained from the correlation approach. Given that I know the madelon data has 20 important (or redundant) features and 480 noise, I felt comfortable with the approach that produced exactly 20 features.\n",
    "\n",
    "### Modeling ###\n",
    "\n",
    "***Description of approach:***\n",
    "\n",
    "In alignment with the benchmarking approach, I modeled the sample on these four models: Logistic Regression, Decision Tree, K Nearest Neighbors, and Support Vector Classifier.\n",
    "\n",
    "The full steps of modeling:\n",
    "1. Train Test Split\n",
    "2. Min Max Scaler\n",
    "3. Deskewing (Boxcox)\n",
    "4. PCA (5 components)\n",
    "5. Standard Scaler\n",
    "6. Model\n",
    "\n",
    "Steps 5 and 6 were built into a pipeline and gridsearched on to tune hyperparameters.\n",
    "\n",
    "The best performing model was K Nearest Neighbors with a ROC AUC Score of .889 and Log Loss of 3.852.\n",
    "\n",
    "### Modeling Scores ###\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "*ROC AUC Score:* 0.619\n",
    "\n",
    "*Log Loss:* 13.151\n",
    "\n",
    "**Decision Tree:**\n",
    "\n",
    "*ROC AUC Score:* 0.773\n",
    "\n",
    "*Log Loss:* 7.838\n",
    "\n",
    "**K Nearest Neighbors:**\n",
    "\n",
    "*ROC AUC Score:* 0.889\n",
    "\n",
    "*Log Loss:* 3.852\n",
    "\n",
    "**Support Vector Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.800\n",
    "\n",
    "*Log Loss:* 6.907\n",
    "\n",
    "_________\n",
    "\n",
    "## Josh Cook Data Set - Subset ##\n",
    "\n",
    "### Benchmarking ###\n",
    "\n",
    "***Description of Approach:***\n",
    "\n",
    "Built naive models to get a prediction benchmark for our classification problem. The models used are:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier\n",
    "3. K Nearest Neighbors\n",
    "4. Support Vector Classifier\n",
    "\n",
    "The best performing model was a close race between Decision Tree Classifier and Support Vector Classifier.\n",
    "\n",
    "Decision Tree Classifier had an ROC AUC Score of .629 and Log Loss of 12.821.\n",
    "\n",
    "Support Vector Classifier had an ROC AUC Score of .624 and Log Loss of 12.978.\n",
    "\n",
    "### Benchmarking Scores\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "*ROC AUC Score:* 0.539\n",
    "\n",
    "*Log Loss:* 15.909\n",
    "\n",
    "**Decision Tree Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.629\n",
    "\n",
    "*Log Loss:* 12.821\n",
    "\n",
    "**K Nearest Neighbors:**\n",
    "\n",
    "*ROC AUC Score:* 0.600\n",
    "\n",
    "*Log Loss:* 13.816\n",
    "\n",
    "**Support Vector Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.624\n",
    "\n",
    "*Log Loss:* 12.978\n",
    "\n",
    "### Feature Selection ###\n",
    "\n",
    "*** Description of Notebook: ***\n",
    "\n",
    "In this approach, I conduct several methods of feature selection. These methods include both supervised and unsupervised approaches.\n",
    "\n",
    "Contents:\n",
    "1. Data Import\n",
    "2. Unsupervised Approach\n",
    "    - Looked at correlation (.corr()) between features and returned those features that are highly correlated ( > 0.5) with at least one other feature.\n",
    "    - The result was 20 features: \n",
    "        - 'feat_257', 'feat_269', 'feat_308', 'feat_315', 'feat_336', 'feat_341', 'feat_395', 'feat_504', 'feat_526', 'feat_639', 'feat_681', 'feat_701', 'feat_724', 'feat_736', 'feat_769', 'feat_808', 'feat_829', 'feat_867', 'feat_920', 'feat_956'\n",
    "    - Visualizations of those 20 features\n",
    "3. Supervised Approaches\n",
    "    - Select from Model w/ Logistic Regression (L2 penalty)\n",
    "    - Select from Model w/ Logistic Regression (L1 penalty)\n",
    "    - SelectKBest w/ k = 20\n",
    "    - RFE w/ RandomForestClassifier as estimator\n",
    "    - These approaches all gave me various feature importances and coefficients with little consistency across them.\n",
    "    \n",
    "**Results:** I decided to use the 20 features gained from the correlation approach. Given that I know the madelon data has 20 important (or redundant) features and 480 noise, I felt comfortable with the approach that produced exactly 20 features.\n",
    "\n",
    "### Modeling ###\n",
    "\n",
    "***Description of notebook:***\n",
    "\n",
    "In alignment with the benchmarking approach, I modeled the sample on these four models: Logistic Regression, Decision Tree, K Nearest Neighbors, and Support Vector Classifier.\n",
    "\n",
    "The full steps of modeling:\n",
    "1. Train Test Split\n",
    "2. Min Max Scaler\n",
    "3. Deskewing (Boxcox)\n",
    "4. PCA (5 components)\n",
    "5. Standard Scaler\n",
    "6. Model\n",
    "\n",
    "Steps 5 and 6 were built into a pipeline and gridsearched on to tune hyperparameters.\n",
    "\n",
    "The best performing model was K Nearest Neighbors with an ROC AUC Score of .744 and Log Loss of 8.844.\n",
    "\n",
    "### Modeling Results ###\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "*ROC AUC Score:* 0.588\n",
    "\n",
    "*Log Loss:* 14.208\n",
    "\n",
    "**Decision Tree:**\n",
    "\n",
    "*ROC AUC Score:* 0.658\n",
    "\n",
    "*Log Loss:* 11.800\n",
    "\n",
    "**K Nearest Neighbors:**\n",
    "\n",
    "*ROC AUC Score:* 0.744\n",
    "\n",
    "*Log Loss:* 8.844\n",
    "\n",
    "**Support Vector Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.711\n",
    "\n",
    "*Log Loss:* 9.969\n",
    "\n",
    "_________________________________________\n",
    "\n",
    "## Josh Cook Data Set - Full Set ##\n",
    "\n",
    "### Benchmarking ###\n",
    "\n",
    "***Description of Approach:***\n",
    "\n",
    "Built naive models to get a prediction benchmark for our classification problem. The models used are:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. Decision Tree Classifier\n",
    "3. K Nearest Neighbors\n",
    "4. Support Vector Classifier\n",
    "\n",
    "The best performing model was a close race between Decision Tree Classifier and Support Vector Classifier.\n",
    "\n",
    "Decision Tree Classifier had an ROC AUC Score of .644 and Log Loss of 12.298.\n",
    "\n",
    "Support Vector Classifier had an ROC AUC Score of .624 and Log Loss of 12.978.\n",
    "\n",
    "### Benchmarking Scores\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "*ROC AUC Score:* 0.539\n",
    "\n",
    "*Log Loss:* 15.909\n",
    "\n",
    "**Decision Tree Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.644\n",
    "\n",
    "*Log Loss:* 12.298\n",
    "\n",
    "**K Nearest Neighbors:**\n",
    "\n",
    "*ROC AUC Score:*  0.600\n",
    "\n",
    "*Log Loss:* 13.816\n",
    "\n",
    "**Support Vector Classifier:**\n",
    "\n",
    "*ROC AUC Score:* 0.624\n",
    "\n",
    "*Log Loss:* 12.978\n",
    "\n",
    "### Feature Selection ###\n",
    "\n",
    "*** Description of approach: ***\n",
    "\n",
    "In this approach, I conduct several methods of feature selection. These methods include both supervised and unsupervised approaches. I've used only a sample of the data to conduct feature selection, as the dataset is too large to run this methods on the full 200,000 rows.\n",
    "\n",
    "Contents:\n",
    "1. Data Import\n",
    "2. Unsupervised Approach\n",
    "    - Looked at correlation (.corr()) between features and returned those features that are highly correlated ( > 0.5) with at least one other feature.\n",
    "    - The result was 20 features: \n",
    "        - 'feat_257', 'feat_269', 'feat_308', 'feat_315', 'feat_336', 'feat_341', 'feat_395', 'feat_504', 'feat_526', 'feat_639', 'feat_681', 'feat_701', 'feat_724', 'feat_736', 'feat_769', 'feat_808', 'feat_829', 'feat_867', 'feat_920', 'feat_956'\n",
    "    - Visualizations of those 20 features\n",
    "3. Supervised Approaches\n",
    "    - Select from Model w/ Logistic Regression (L2 penalty)\n",
    "    - Select from Model w/ Logistic Regression (L1 penalty)\n",
    "    - SelectKBest w/ k = 20\n",
    "    - RFE w/ RandomForestClassifier as estimator\n",
    "    - These approaches all gave me various feature importances and coefficients with little consistency across them.\n",
    "    \n",
    "**Results:** I decided to use the 20 features gained from the correlation approach. Given that I know the madelon data has 20 important (or redundant) features and 480 noise, I felt comfortable with the approach that produced exactly 20 features.\n",
    "\n",
    "### Modeling ###\n",
    "\n",
    "***Description of notebook:***\n",
    "\n",
    "I ran my best model fit and scored from the sample of the data from Josh Cook's database on the full dataset.\n",
    "\n",
    "The full steps of modeling:\n",
    "1. Train Test Split\n",
    "2. Min Max Scaler\n",
    "3. Deskewing (Boxcox)\n",
    "4. PCA (5 components)\n",
    "5. Standard Scaler\n",
    "6. Model\n",
    "\n",
    "Steps 5 and 6 were built into a pipeline and gridsearched on to tune hyperparameters.\n",
    "\n",
    "As mentioned, K Nearest Neighbors performed best. Results below:\n",
    "\n",
    "### Results ###\n",
    "\n",
    "**K Nearest Neighbors:**\n",
    "\n",
    "*ROC AUC Score:* 0.757\n",
    "\n",
    "*Log Loss:* 8.389\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

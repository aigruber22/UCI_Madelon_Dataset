{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel, RFE, RFECV, SelectKBest, chi2, f_classif\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.metrics import roc_auc_score, classification_report, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./data/madelon_train.data', \n",
    "                         sep = ' ', header = None).drop(500, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv('./data/madelon_train.labels', sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_data = pd.read_csv('./data/madelon_valid.data', \n",
    "                         sep = ' ', header = None).drop(500, axis = 1)\n",
    "val_labels = pd.read_csv('./data/madelon_valid.labels', sep = ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./data/madelon_test.data', \n",
    "                         sep = ' ', header = None).drop(500, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data['target'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_data['target'] = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = train_data.append(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub1 = df.sample((int(.1*(len(df)))), random_state = 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 501)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub2 = df.sample((int(.1*(len(df)))), random_state = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 501)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub3 = df.sample((int(.1*(len(df)))), random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 501)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fdd3ef21898>]], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEHCAYAAABRF9YCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdd1c1f2550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub1.hist(column = 'target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_neg1 = 123/260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_pos1 = 137/260"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    137\n",
       "-1    123\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub1['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = sub1[sub1.columns[0:500]]\n",
    "target = sub1[sub1.columns[500]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260, 500)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(260,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size = .2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/morestats.py:901: RuntimeWarning: invalid value encountered in subtract\n",
      "  llf -= N / 2.0 * np.log(np.sum((y - y_mean)**2. / N, axis=0))\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/morestats.py:901: RuntimeWarning: overflow encountered in square\n",
      "  llf -= N / 2.0 * np.log(np.sum((y - y_mean)**2. / N, axis=0))\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/optimize/optimize.py:1850: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp2 = (x - v) * (fx - fw)\n",
      "/opt/conda/lib/python3.6/site-packages/scipy/optimize/optimize.py:1855: RuntimeWarning: invalid value encountered in absolute\n",
      "  tmp2 = numpy.abs(tmp2)\n"
     ]
    }
   ],
   "source": [
    "X_train_bc = pd.DataFrame()\n",
    "X_test_bc = pd.DataFrame()\n",
    "for col in X_train.columns:\n",
    "    box_cox_trans_train, lmbda = boxcox(X_train[col])\n",
    "    box_cox_trans_test = boxcox(X_test[col], lmbda)\n",
    "    X_train_bc[col] = pd.Series(box_cox_trans_train)\n",
    "    X_test_bc[col] = pd.Series(box_cox_trans_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select From Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression as estimator with L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "lr_l2 = LogisticRegression(C = .0001)\n",
    "sfm1 = SelectFromModel(lr_l2, threshold = 'mean')\n",
    "pipe_sfm_lr = Pipeline([\n",
    "    ('scaler', scaler),  \n",
    "    ('sfm', sfm1) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sfm', SelectFromModel(estimator=LogisticRegression(C=0.0001, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False),\n",
       "        prefit=False, threshold='mean'))])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_sfm_lr.fit(X_train_bc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   3,   4,   5,   9,  11,  14,  16,  19,  21,  22,  28,  29,\n",
       "        30,  31,  32,  33,  37,  39,  43,  46,  48,  49,  51,  52,  53,\n",
       "        55,  57,  60,  62,  64,  67,  74,  77,  78,  81,  85,  86,  89,\n",
       "        98,  99, 100, 105, 108, 113, 120, 121, 127, 128, 129, 133, 134,\n",
       "       135, 138, 141, 143, 148, 149, 152, 156, 157, 158, 162, 164, 165,\n",
       "       166, 167, 169, 174, 175, 176, 177, 181, 189, 190, 191, 196, 197,\n",
       "       199, 202, 203, 204, 206, 207, 208, 212, 215, 218, 220, 221, 222,\n",
       "       224, 225, 227, 228, 233, 237, 238, 241, 243, 246, 248, 253, 258,\n",
       "       260, 263, 265, 267, 268, 271, 272, 273, 276, 278, 283, 284, 285,\n",
       "       286, 289, 293, 294, 295, 296, 297, 301, 304, 305, 307, 310, 313,\n",
       "       316, 318, 319, 321, 322, 324, 326, 328, 329, 330, 331, 336, 337,\n",
       "       338, 339, 341, 345, 352, 354, 365, 368, 369, 374, 375, 378, 380,\n",
       "       382, 383, 386, 388, 399, 400, 401, 406, 409, 412, 413, 414, 415,\n",
       "       416, 417, 420, 421, 425, 426, 428, 429, 431, 432, 434, 437, 439,\n",
       "       441, 442, 445, 446, 449, 450, 451, 452, 453, 455, 459, 461, 466,\n",
       "       467, 468, 469, 472, 475, 477, 485, 491, 492, 493, 495, 496, 497])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm1_feats = np.where(sfm1.get_support())[0]\n",
    "sfm1_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sfm1_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfm1_feat_coef_ind = np.argsort(np.abs(sfm1.estimator_.coef_))[0][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.20818235e-03,   4.17028567e-03,   3.42803678e-03,\n",
       "         3.40956947e-03,   2.21754458e-03,   2.14666481e-03,\n",
       "         2.09637130e-03,   2.03920931e-03,   2.00227480e-03,\n",
       "         1.98684275e-03,   1.95823074e-03,   1.92521143e-03,\n",
       "         1.77081015e-03,   1.73789920e-03,   1.70397409e-03,\n",
       "         1.70307732e-03,   1.65881048e-03,   1.60145497e-03,\n",
       "         1.58566737e-03,   1.57189441e-03,   1.56927532e-03,\n",
       "         1.55660464e-03,   1.54021947e-03,   1.53249673e-03,\n",
       "         1.49511604e-03,   1.48814272e-03,   1.47292466e-03,\n",
       "         1.47025713e-03,   1.46551087e-03,   1.46048105e-03,\n",
       "         1.43259336e-03,   1.42299100e-03,   1.37196907e-03,\n",
       "         1.35265517e-03,   1.34587169e-03,   1.33871502e-03,\n",
       "         1.32763485e-03,   1.32615046e-03,   1.32253132e-03,\n",
       "         1.31600257e-03,   1.30437311e-03,   1.30117368e-03,\n",
       "         1.29895395e-03,   1.28849935e-03,   1.28830475e-03,\n",
       "         1.28598064e-03,   1.28455445e-03,   1.27388346e-03,\n",
       "         1.25773559e-03,   1.23469722e-03,   1.23218169e-03,\n",
       "         1.22235990e-03,   1.19567550e-03,   1.18708265e-03,\n",
       "         1.18201682e-03,   1.16557128e-03,   1.16063453e-03,\n",
       "         1.15372760e-03,   1.15189553e-03,   1.13324580e-03,\n",
       "         1.12546218e-03,   1.11556763e-03,   1.10909529e-03,\n",
       "         1.09886447e-03,   1.09834760e-03,   1.09201021e-03,\n",
       "         1.08517639e-03,   1.07982991e-03,   1.07626217e-03,\n",
       "         1.06392169e-03,   1.05746446e-03,   1.05725428e-03,\n",
       "         1.05692499e-03,   1.05659214e-03,   1.05186120e-03,\n",
       "         1.05167971e-03,   1.04627096e-03,   1.04246075e-03,\n",
       "         1.02856844e-03,   1.02690429e-03,   1.01392038e-03,\n",
       "         1.01293710e-03,   1.01187944e-03,   1.00903035e-03,\n",
       "         1.00825235e-03,   1.00741028e-03,   1.00054544e-03,\n",
       "         1.00017897e-03,   9.98028700e-04,   9.96926354e-04,\n",
       "         9.82800042e-04,   9.78962870e-04,   9.69954878e-04,\n",
       "         9.64158957e-04,   9.63302293e-04,   9.62472463e-04,\n",
       "         9.58363253e-04,   9.57624789e-04,   9.50766322e-04,\n",
       "         9.39286825e-04,   9.38805806e-04,   9.36351345e-04,\n",
       "         9.31355459e-04,   9.28725102e-04,   9.27777254e-04,\n",
       "         9.26275467e-04,   9.16122149e-04,   9.12080745e-04,\n",
       "         9.10398529e-04,   9.08117283e-04,   9.07561560e-04,\n",
       "         9.05140326e-04,   9.03740585e-04,   9.01923116e-04,\n",
       "         8.98036509e-04,   8.86296426e-04,   8.82882105e-04,\n",
       "         8.79920915e-04,   8.79465542e-04,   8.78335238e-04,\n",
       "         8.74471013e-04,   8.69697219e-04,   8.66396163e-04,\n",
       "         8.62113838e-04,   8.52246368e-04,   8.50562440e-04,\n",
       "         8.49427667e-04,   8.48103101e-04,   8.47068851e-04,\n",
       "         8.43013783e-04,   8.41370059e-04,   8.38529518e-04,\n",
       "         8.38021853e-04,   8.30246119e-04,   8.29691416e-04,\n",
       "         8.23796634e-04,   8.12955126e-04,   8.12620946e-04,\n",
       "         8.11189119e-04,   8.09607270e-04,   8.00221710e-04,\n",
       "         7.99796577e-04,   7.98297676e-04,   7.97568605e-04,\n",
       "         7.94927821e-04,   7.93921597e-04,   7.90551461e-04,\n",
       "         7.87363156e-04,   7.86155410e-04,   7.84470301e-04,\n",
       "         7.81877502e-04,   7.81212380e-04,   7.78334034e-04,\n",
       "         7.75970055e-04,   7.72934714e-04,   7.69586712e-04,\n",
       "         7.69441169e-04,   7.57947142e-04,   7.57892206e-04,\n",
       "         7.56023478e-04,   7.49286796e-04,   7.48870158e-04,\n",
       "         7.48008453e-04,   7.46108377e-04,   7.46097034e-04,\n",
       "         7.41026370e-04,   7.38087355e-04,   7.22486866e-04,\n",
       "         7.22050235e-04,   7.19187595e-04,   7.16731646e-04,\n",
       "         7.13685669e-04,   7.11000479e-04,   7.10019726e-04,\n",
       "         7.09841596e-04,   7.07794035e-04,   6.99843207e-04,\n",
       "         6.95832369e-04,   6.94252868e-04,   6.92404175e-04,\n",
       "         6.88635604e-04,   6.86828721e-04,   6.86499584e-04,\n",
       "         6.83074144e-04,   6.81800137e-04,   6.79122624e-04,\n",
       "         6.79084710e-04,   6.74570813e-04,   6.72192881e-04,\n",
       "         6.67814193e-04,   6.62085571e-04,   6.58777064e-04,\n",
       "         6.58290697e-04,   6.53474233e-04,   6.53065945e-04,\n",
       "         6.51846616e-04,   6.46957056e-04,   6.45169118e-04,\n",
       "         6.41011825e-04,   6.40344631e-04,   6.35324316e-04,\n",
       "         6.34043913e-04,   6.24245656e-04,   6.20510959e-04,\n",
       "         6.19446603e-04,   6.14579752e-04,   6.14244945e-04,\n",
       "         6.14233244e-04,   5.95599470e-04,   5.95528878e-04,\n",
       "         5.91453845e-04,   5.83431863e-04,   5.77336666e-04,\n",
       "         5.73338576e-04,   5.69496691e-04,   5.69172169e-04,\n",
       "         5.65257281e-04,   5.63217315e-04,   5.63186553e-04,\n",
       "         5.62047795e-04,   5.61827315e-04,   5.59342468e-04,\n",
       "         5.58556119e-04,   5.56770160e-04,   5.55326961e-04,\n",
       "         5.53159753e-04,   5.46717020e-04,   5.27508045e-04,\n",
       "         5.26754051e-04,   5.23673458e-04,   5.23550584e-04,\n",
       "         5.22101587e-04,   5.21357104e-04,   5.20789130e-04,\n",
       "         5.14928775e-04,   5.10696050e-04,   5.07025543e-04,\n",
       "         5.05289028e-04,   4.92157908e-04,   4.92136172e-04,\n",
       "         4.88165990e-04,   4.88133108e-04,   4.88081076e-04,\n",
       "         4.84314680e-04,   4.83755394e-04,   4.80319813e-04,\n",
       "         4.71648923e-04,   4.70509305e-04,   4.66647240e-04,\n",
       "         4.64257157e-04,   4.63814515e-04,   4.63616302e-04,\n",
       "         4.61605469e-04,   4.59788331e-04,   4.59379863e-04,\n",
       "         4.58712195e-04,   4.57444585e-04,   4.55579880e-04,\n",
       "         4.55227854e-04,   4.49261227e-04,   4.43618636e-04,\n",
       "         4.41987104e-04,   4.38941008e-04,   4.34291704e-04,\n",
       "         4.34015088e-04,   4.30452149e-04,   4.28631343e-04,\n",
       "         4.26756535e-04,   4.26333318e-04,   4.26106221e-04,\n",
       "         4.25946668e-04,   4.24716591e-04,   4.20087133e-04,\n",
       "         4.17907629e-04,   4.10392617e-04,   4.08278165e-04,\n",
       "         4.05373477e-04,   4.04941183e-04,   3.96942154e-04,\n",
       "         3.94406767e-04,   3.94161310e-04,   3.92957911e-04,\n",
       "         3.92953629e-04,   3.91974937e-04,   3.89553933e-04,\n",
       "         3.89048566e-04,   3.88267937e-04,   3.87926649e-04,\n",
       "         3.86733778e-04,   3.85425373e-04,   3.82909692e-04,\n",
       "         3.82214617e-04,   3.82103094e-04,   3.80336152e-04,\n",
       "         3.80188492e-04,   3.79918583e-04,   3.76980073e-04,\n",
       "         3.71684325e-04,   3.71282699e-04,   3.70284124e-04,\n",
       "         3.70013434e-04,   3.68773264e-04,   3.67456825e-04,\n",
       "         3.63959575e-04,   3.62252868e-04,   3.61610296e-04,\n",
       "         3.60898517e-04,   3.60729580e-04,   3.57894439e-04,\n",
       "         3.57641734e-04,   3.54122895e-04,   3.44559793e-04,\n",
       "         3.44036502e-04,   3.40587668e-04,   3.39536029e-04,\n",
       "         3.38447029e-04,   3.37930761e-04,   3.37321588e-04,\n",
       "         3.35611454e-04,   3.33493656e-04,   3.33135074e-04,\n",
       "         3.32644367e-04,   3.31258423e-04,   3.31201397e-04,\n",
       "         3.30482371e-04,   3.27158783e-04,   3.26562066e-04,\n",
       "         3.26509932e-04,   3.26025823e-04,   3.24346839e-04,\n",
       "         3.24105295e-04,   3.21994136e-04,   3.20562221e-04,\n",
       "         3.18647865e-04,   3.17967959e-04,   3.17442727e-04,\n",
       "         3.14037565e-04,   3.12929643e-04,   3.10251630e-04,\n",
       "         3.00844210e-04,   3.00316179e-04,   2.97424983e-04,\n",
       "         2.97085455e-04,   2.87225320e-04,   2.85182052e-04,\n",
       "         2.84122710e-04,   2.84081027e-04,   2.82360964e-04,\n",
       "         2.79144383e-04,   2.78537774e-04,   2.78061049e-04,\n",
       "         2.75685310e-04,   2.75047096e-04,   2.73617796e-04,\n",
       "         2.72333730e-04,   2.68673672e-04,   2.67157612e-04,\n",
       "         2.63326614e-04,   2.62327393e-04,   2.61714509e-04,\n",
       "         2.61326505e-04,   2.60610572e-04,   2.59954769e-04,\n",
       "         2.58492865e-04,   2.52911074e-04,   2.50717703e-04,\n",
       "         2.49090673e-04,   2.48566558e-04,   2.46566127e-04,\n",
       "         2.43005022e-04,   2.42169272e-04,   2.38985890e-04,\n",
       "         2.38067474e-04,   2.36450570e-04,   2.36270352e-04,\n",
       "         2.35785105e-04,   2.34029986e-04,   2.32758102e-04,\n",
       "         2.29420305e-04,   2.25784464e-04,   2.24431482e-04,\n",
       "         2.24298873e-04,   2.21210467e-04,   2.18733601e-04,\n",
       "         2.17892865e-04,   2.11226857e-04,   2.10290632e-04,\n",
       "         2.06704079e-04,   2.03293329e-04,   1.96653782e-04,\n",
       "         1.95585596e-04,   1.92821379e-04,   1.92181154e-04,\n",
       "         1.91913025e-04,   1.88203573e-04,   1.86230708e-04,\n",
       "         1.84700835e-04,   1.83227849e-04,   1.81807253e-04,\n",
       "         1.81564387e-04,   1.79124225e-04,   1.78190699e-04,\n",
       "         1.77848862e-04,   1.76311486e-04,   1.71490982e-04,\n",
       "         1.71350104e-04,   1.68564705e-04,   1.67009124e-04,\n",
       "         1.63430850e-04,   1.60568266e-04,   1.59675843e-04,\n",
       "         1.57780507e-04,   1.56425401e-04,   1.55865671e-04,\n",
       "         1.54356988e-04,   1.50653524e-04,   1.50615710e-04,\n",
       "         1.42134344e-04,   1.41606874e-04,   1.40060939e-04,\n",
       "         1.39551565e-04,   1.39138484e-04,   1.33629924e-04,\n",
       "         1.28529149e-04,   1.27667027e-04,   1.27292155e-04,\n",
       "         1.26848289e-04,   1.26520382e-04,   1.26392495e-04,\n",
       "         1.22555613e-04,   1.16817088e-04,   1.12629056e-04,\n",
       "         1.09143615e-04,   1.07215229e-04,   1.04600048e-04,\n",
       "         1.04503916e-04,   1.04092948e-04,   1.03346479e-04,\n",
       "         1.00876485e-04,   9.97494801e-05,   9.68149971e-05,\n",
       "         9.08749332e-05,   8.77638008e-05,   8.71581910e-05,\n",
       "         8.33840736e-05,   8.23695851e-05,   8.22678674e-05,\n",
       "         8.10643033e-05,   8.07205790e-05,   8.00604128e-05,\n",
       "         7.87422848e-05,   7.81399882e-05,   7.74162206e-05,\n",
       "         7.38313093e-05,   7.03927673e-05,   7.03166555e-05,\n",
       "         6.88766539e-05,   6.79224412e-05,   6.73614543e-05,\n",
       "         6.67694760e-05,   6.65775423e-05,   6.37779447e-05,\n",
       "         6.36283158e-05,   6.28843045e-05,   6.19532132e-05,\n",
       "         6.16032958e-05,   6.11702222e-05,   6.04490617e-05,\n",
       "         5.50976696e-05,   5.40737152e-05,   5.39718533e-05,\n",
       "         5.03080628e-05,   5.01482697e-05,   4.97245069e-05,\n",
       "         4.94867860e-05,   4.44531003e-05,   4.37952665e-05,\n",
       "         4.31631190e-05,   4.08270311e-05,   3.67950747e-05,\n",
       "         3.57310080e-05,   3.53010864e-05,   3.52587548e-05,\n",
       "         3.49011322e-05,   3.32331517e-05,   2.47250872e-05,\n",
       "         2.32117692e-05,   2.26141369e-05,   1.97353713e-05,\n",
       "         1.92611861e-05,   1.44843769e-05,   1.05608299e-05,\n",
       "         1.03848720e-05,   1.03475964e-05,   8.30820424e-06,\n",
       "         7.44900413e-06,   6.42404754e-06,   5.26755698e-06,\n",
       "         3.23739988e-06,   2.97869609e-06])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm1_feat_coef_val = np.sort(np.abs(sfm1.estimator_.coef_))[0][::-1]\n",
    "sfm1_feat_coef_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduced the space and size, but doesn't say anything about the top 20 features specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression as estimator with L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "lr_l1 = LogisticRegression(penalty='l1', C = .1)\n",
    "sfm2 = SelectFromModel(lr_l1)\n",
    "pipe_sfm_lr_l1 = Pipeline([\n",
    "    ('scaler', scaler),  \n",
    "    ('sfm', sfm2) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('sfm', SelectFromModel(estimator=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "        prefit=False, threshold=None))])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_sfm_lr_l1.fit(X_train_bc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   5,   9,  11,  33, 105, 121, 129, 165, 191, 212, 224, 237,\n",
       "       241, 260, 268, 278, 283, 285, 289, 307, 319, 331, 354, 369, 378,\n",
       "       388, 399, 421, 428, 431, 446, 485, 491, 495])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm2_feats = np.where(sfm2.get_support())[0]\n",
    "sfm2_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sfm2_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfm2_feat_coef_ind = np.argsort(np.abs(sfm2.estimator_.coef_))[0][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65408697,  0.18531013,  0.18190207,  0.17747564,  0.14746289,\n",
       "        0.14522478,  0.14182102,  0.13159951,  0.11720184,  0.08680062,\n",
       "        0.06802265,  0.0678929 ,  0.05869917,  0.05823517,  0.05785975,\n",
       "        0.05136264,  0.0499949 ,  0.04730833,  0.04081627,  0.04025121,\n",
       "        0.03958758,  0.03817224,  0.0361136 ,  0.03305697,  0.03054276,\n",
       "        0.02916907,  0.0270442 ,  0.02647882,  0.02467327,  0.02406629,\n",
       "        0.02333551,  0.0204497 ,  0.01006961,  0.00527402,  0.00490921,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm2_feat_coef_val = np.sort(np.abs(sfm2.estimator_.coef_))[0][::-1]\n",
    "sfm2_feat_coef_val[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting...L1 penalty is much better for feature selection in general"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ DOCUMENTATION to determine if you can use a regression estimator for a classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** SelectKBest **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "skb = SelectKBest(f_classif, k = 35)\n",
    "pipe_fs_skb = Pipeline([\n",
    "    ('scaler', scaler),  \n",
    "    ('skb', skb) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f_classif vs chi2 vs others\n",
    "- keep in mind different scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('skb', SelectKBest(k=35, score_func=<function f_classif at 0x7fdd1d8eb158>))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_fs_skb.fit(X_train_bc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.83528439e-01,   7.30320175e-01,   9.49602251e-01,\n",
       "         1.66190257e-02,   2.04954483e-01,   6.24578091e-02,\n",
       "         6.63175561e-01,   5.73031496e-01,   4.28581784e-01,\n",
       "         8.56507988e-02,   7.65484894e-01,   2.83495344e-02,\n",
       "         7.67388441e-01,   5.19180489e-01,   1.39086203e-01,\n",
       "         7.08837736e-01,   2.77413396e-01,   7.23854282e-01,\n",
       "         8.11644701e-01,   3.17423255e-01,   8.20207891e-01,\n",
       "         2.93872925e-01,   2.53144100e-01,   4.32966837e-01,\n",
       "         7.99123813e-01,   4.80153092e-01,   9.21018824e-01,\n",
       "         5.66202185e-01,   1.76983837e-01,   3.10611128e-01,\n",
       "         1.04852039e-01,   2.53427245e-01,   2.87769443e-01,\n",
       "         4.57017035e-03,   7.14437616e-01,   5.72407358e-01,\n",
       "         9.82701505e-01,   8.29916856e-02,   4.14031413e-01,\n",
       "         2.88356643e-01,   9.11317814e-01,   6.19687408e-01,\n",
       "         8.39053061e-01,   2.20341932e-01,   3.98629782e-01,\n",
       "         9.13439302e-01,   1.56149981e-01,   5.26756046e-01,\n",
       "         8.09096831e-02,   2.40249657e-01,   7.33106589e-01,\n",
       "         3.12318849e-01,   3.32411754e-01,   2.69412325e-01,\n",
       "         9.91165048e-01,   1.09693390e-01,   4.92374814e-01,\n",
       "         1.51898797e-01,   6.26050138e-01,   5.11267326e-01,\n",
       "         1.67003093e-01,   4.49836617e-01,   2.05122102e-01,\n",
       "         7.89494631e-01,   4.69411233e-07,   5.94375470e-01,\n",
       "         4.27732957e-01,   3.59174216e-01,   4.26858647e-01,\n",
       "         8.74874320e-01,   6.35647882e-01,   6.03264229e-01,\n",
       "         9.24026628e-01,   5.96911594e-01,   3.85371036e-01,\n",
       "         8.15856275e-01,   7.19433137e-01,   2.87109055e-01,\n",
       "         3.89428752e-01,   4.78097494e-01,   6.47057409e-01,\n",
       "         3.44893359e-01,   6.98558846e-01,   7.00244500e-01,\n",
       "         7.64908469e-01,   3.32297489e-01,   2.50000920e-01,\n",
       "         8.41218793e-01,   6.72103920e-01,   2.49765811e-01,\n",
       "         9.20097182e-01,   5.52353947e-01,   8.66382766e-01,\n",
       "         8.46815818e-01,   7.36328947e-01,   5.97888519e-01,\n",
       "         7.71512044e-01,   7.55045371e-01,   6.94094059e-02,\n",
       "         1.97323139e-01,   2.74568940e-01,   9.67778854e-01,\n",
       "         6.02565842e-01,   5.40266179e-01,   8.91798420e-01,\n",
       "         2.34034179e-03,   6.73177678e-01,   5.21643325e-01,\n",
       "         1.49856464e-01,   5.34267256e-01,   9.30460124e-01,\n",
       "         6.32686483e-01,   6.05239610e-01,   2.09666071e-01,\n",
       "         7.84368515e-01,   4.64973291e-01,   9.52666191e-01,\n",
       "         9.89393783e-01,   7.87295289e-01,   9.45587332e-01,\n",
       "         4.69030562e-02,   1.47062396e-02,   9.23473353e-01,\n",
       "         7.03198250e-01,   7.12117534e-01,   7.84681658e-01,\n",
       "         9.59302374e-01,   2.07399723e-01,   3.64713039e-03,\n",
       "         5.89312306e-02,   8.57459561e-01,   7.54967733e-01,\n",
       "         5.80087242e-01,   2.01765995e-01,   2.31430269e-01,\n",
       "         6.23587404e-02,   8.26411373e-01,   8.99014730e-01,\n",
       "         3.36564720e-01,   6.81398167e-01,   7.37646825e-01,\n",
       "         1.46950190e-01,   9.33157337e-01,   2.65025743e-01,\n",
       "         4.23786832e-01,   8.66763412e-01,   5.17592414e-01,\n",
       "         4.77184849e-01,   3.68428974e-01,   3.35022894e-01,\n",
       "         4.79480058e-01,   8.89435236e-01,   3.56414508e-01,\n",
       "         6.47531572e-01,   5.34260216e-01,   9.43884519e-01,\n",
       "         2.91239955e-01,   1.73082847e-01,   2.70062241e-01,\n",
       "         8.95234389e-01,   5.46349345e-01,   9.47714873e-01,\n",
       "         2.89240383e-01,   4.90769927e-01,   3.15872329e-01,\n",
       "         1.80709757e-02,   6.49858449e-02,   4.17793835e-02,\n",
       "         9.57990381e-01,   3.47965509e-01,   6.31294986e-01,\n",
       "         7.26582553e-01,   6.39918946e-01,   8.46750151e-01,\n",
       "         2.52531762e-01,   3.36314126e-02,   1.50978097e-01,\n",
       "         3.75461568e-01,   5.25535351e-01,   9.22347064e-01,\n",
       "         6.02596916e-01,   1.84895685e-01,   7.27414805e-01,\n",
       "         5.73516430e-01,   9.61815132e-01,   4.83581273e-01,\n",
       "         5.28945888e-01,   6.00227330e-01,   9.99103350e-01,\n",
       "         2.65692910e-02,   2.17239032e-01,   2.36475066e-02,\n",
       "         9.84582897e-01,   4.84650942e-01,   5.57320555e-01,\n",
       "         9.25116896e-01,   1.37233515e-01,   3.41253066e-01,\n",
       "         6.88375112e-01,   1.21247674e-01,   6.17657397e-01,\n",
       "         6.56157503e-01,   3.39155487e-01,   3.10178687e-01,\n",
       "         3.00521459e-01,   7.02830883e-01,   2.28796562e-01,\n",
       "         2.59571867e-01,   9.87148499e-02,   7.37252924e-01,\n",
       "         6.35599527e-01,   8.10931993e-01,   1.14781352e-02,\n",
       "         4.35469629e-01,   8.26079244e-01,   1.30275991e-01,\n",
       "         7.10470911e-01,   8.46565283e-01,   3.74051943e-01,\n",
       "         5.53571239e-01,   3.76131764e-01,   2.05545727e-01,\n",
       "         2.54440477e-01,   7.97941912e-01,   2.44500507e-02,\n",
       "         6.47891876e-02,   9.15218590e-01,   2.65447197e-01,\n",
       "         3.42154098e-01,   4.74842887e-01,   5.46615839e-01,\n",
       "         4.40804999e-01,   6.20110258e-01,   7.11370801e-02,\n",
       "         7.79857639e-01,   6.35703639e-01,   4.61326601e-01,\n",
       "         1.41179182e-01,   2.26775016e-01,   8.65915130e-01,\n",
       "         4.29221594e-01,   2.88240039e-10,   9.78658992e-01,\n",
       "         2.80922780e-01,   8.78192871e-01,   6.54954834e-01,\n",
       "         3.82130622e-01,   8.90199399e-01,   9.39332495e-02,\n",
       "         9.24664944e-01,   8.77650788e-01,   7.89534741e-01,\n",
       "         6.26260757e-01,   9.77072493e-02,   5.15385572e-01,\n",
       "         5.90379258e-01,   8.16357358e-01,   6.18121903e-01,\n",
       "         3.83447594e-01,   9.32234284e-01,   3.85506960e-02,\n",
       "         6.32201974e-01,   9.32457167e-01,   2.15792688e-01,\n",
       "         8.43450675e-01,   2.44080609e-01,   7.51304124e-01,\n",
       "         2.84286608e-01,   2.91474487e-02,   6.54021688e-01,\n",
       "         4.56363787e-01,   2.93565108e-02,   2.26709281e-01,\n",
       "         1.29227553e-01,   8.46370736e-01,   9.50465533e-01,\n",
       "         2.84682897e-01,   7.86631614e-01,   4.26922277e-02,\n",
       "         9.51153663e-01,   6.39667095e-01,   6.41441327e-01,\n",
       "         5.86353697e-01,   3.86185323e-02,   2.13371028e-01,\n",
       "         5.99527257e-03,   1.17915497e-01,   6.86051691e-01,\n",
       "         9.33770073e-01,   5.35320083e-02,   6.68163244e-01,\n",
       "         9.03577669e-01,   8.80886651e-01,   2.75360078e-01,\n",
       "         1.90692757e-01,   3.36392690e-01,   3.25868734e-01,\n",
       "         3.27209806e-02,   4.10125638e-01,   8.72583833e-01,\n",
       "         7.01690241e-01,   3.52518012e-01,   4.62748785e-01,\n",
       "         5.11226177e-01,   2.68253138e-01,   2.23815992e-01,\n",
       "         6.32820156e-01,   1.54072102e-02,   7.52257596e-01,\n",
       "         5.41619629e-01,   1.88404728e-01,   5.87922013e-01,\n",
       "         6.48988703e-01,   2.34035186e-01,   9.26405867e-01,\n",
       "         9.73312877e-01,   6.77321358e-02,   5.06244474e-01,\n",
       "         1.58965657e-01,   5.89227829e-02,   7.75374672e-01,\n",
       "         1.95638841e-01,   6.00047394e-02,   9.80427471e-01,\n",
       "         2.24493033e-01,   9.08880697e-01,   1.04821799e-01,\n",
       "         4.52123763e-01,   3.09837876e-01,   3.13787608e-01,\n",
       "         1.29912951e-01,   2.33014546e-03,   6.93490770e-01,\n",
       "         9.58427424e-01,   7.92330621e-01,   7.15878819e-01,\n",
       "         4.02900000e-07,   1.85539342e-01,   1.25135521e-03,\n",
       "         1.06583382e-01,   9.52691787e-01,   1.26756651e-01,\n",
       "         6.62120599e-01,   7.05907176e-01,   6.37297402e-01,\n",
       "         1.71745691e-01,   4.29951099e-01,   9.95278698e-01,\n",
       "         8.31184849e-01,   5.14054332e-01,   6.85593154e-01,\n",
       "         8.89076235e-01,   2.66900833e-01,   9.82235337e-01,\n",
       "         3.52495482e-02,   8.27564563e-01,   6.45138980e-01,\n",
       "         8.84633911e-01,   9.91382129e-01,   6.94240964e-01,\n",
       "         9.75132222e-01,   7.29812722e-01,   6.07719844e-01,\n",
       "         4.74592245e-01,   4.21123140e-01,   3.56105339e-01,\n",
       "         6.37659330e-01,   6.67425528e-01,   1.61138548e-01,\n",
       "         5.37299842e-03,   5.54192434e-01,   5.95727357e-01,\n",
       "         8.58742784e-01,   5.24810486e-01,   3.49622016e-01,\n",
       "         1.44128719e-01,   8.22455468e-01,   8.12513451e-01,\n",
       "         6.86784678e-02,   4.96848997e-01,   1.16915946e-01,\n",
       "         4.23984268e-01,   3.21365611e-01,   7.35788819e-02,\n",
       "         7.16424038e-01,   4.97534560e-01,   2.19082864e-01,\n",
       "         8.06592659e-01,   1.98401132e-01,   7.90790035e-01,\n",
       "         9.22544818e-01,   7.24030654e-01,   7.00252601e-01,\n",
       "         5.92483669e-01,   5.43871776e-01,   6.75510237e-01,\n",
       "         9.74197057e-01,   7.79638597e-01,   6.85818050e-01,\n",
       "         3.71487684e-03,   2.01694494e-01,   6.56779194e-02,\n",
       "         6.81182514e-01,   5.03701479e-01,   8.59030876e-01,\n",
       "         4.03830781e-01,   2.63994287e-01,   6.41304827e-01,\n",
       "         4.60993060e-01,   3.64680976e-01,   7.69568919e-01,\n",
       "         4.33396442e-01,   1.19746730e-01,   3.05615375e-01,\n",
       "         3.27253039e-01,   3.25205218e-01,   1.42744556e-01,\n",
       "         1.55208805e-01,   5.80466815e-01,   6.54720977e-01,\n",
       "         3.65472444e-01,   2.62581377e-02,   9.31781510e-01,\n",
       "         9.60812765e-01,   6.03263423e-01,   2.29911058e-01,\n",
       "         1.94259439e-01,   6.95171408e-01,   2.04933121e-01,\n",
       "         3.48482667e-01,   9.81112203e-01,   1.38736775e-01,\n",
       "         1.89103715e-01,   5.93455910e-01,   2.65840089e-01,\n",
       "         5.41206239e-01,   5.23882743e-01,   6.23102739e-02,\n",
       "         7.12887947e-01,   1.73910470e-01,   7.70862913e-01,\n",
       "         2.67872709e-01,   8.57367305e-02,   5.15063499e-01,\n",
       "         5.99774043e-01,   9.02591865e-02,   1.23293695e-01,\n",
       "         9.40058280e-01,   5.82980978e-01,   2.82808001e-01,\n",
       "         2.21785713e-01,   2.76016497e-01,   3.37677811e-01,\n",
       "         1.51861523e-01,   7.22141163e-01,   1.29596250e-01,\n",
       "         5.23631860e-01,   4.23774528e-01,   9.89374227e-01,\n",
       "         2.01090252e-01,   8.66506717e-01,   2.31620905e-01,\n",
       "         8.22269590e-01,   7.49856675e-01,   6.12926106e-01,\n",
       "         5.83156357e-01,   1.65195370e-01,   1.60032788e-01,\n",
       "         5.75415446e-02,   1.76315830e-01,   4.22324520e-01,\n",
       "         9.15275550e-01,   6.30333677e-02,   6.08145449e-01,\n",
       "         9.08643108e-01,   4.21075772e-10,   5.55008084e-01,\n",
       "         3.19194393e-01,   9.54577302e-01,   8.53339936e-01,\n",
       "         6.57875362e-01,   5.56313063e-01,   8.40935831e-01,\n",
       "         6.12588462e-01,   6.18754804e-01,   1.00987636e-01,\n",
       "         9.23808737e-01,   7.24867442e-01,   5.90433337e-01,\n",
       "         9.32661946e-01,   9.14748094e-01,   1.39394727e-01,\n",
       "         1.57692402e-01,   1.71355540e-01,   5.75519324e-01,\n",
       "         2.39425185e-01,   4.04130708e-01,   1.91432216e-01,\n",
       "         4.21625519e-01,   5.75704446e-01])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb.pvalues_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False,  True, False, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False,  True, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True,  True, False, False, False, False,\n",
       "       False, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False,  True, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False,  True, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False,  True, False, False, False, False, False, False,  True,\n",
       "       False, False, False, False,  True, False,  True, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False, False, False,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False,  True, False,  True, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False,  True, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "        True, False, False, False, False, False, False,  True, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False], dtype=bool)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  11,  33,  64, 105, 120, 121, 128, 165, 167, 175, 189, 191,\n",
       "       212, 224, 241, 260, 268, 271, 278, 283, 285, 289, 297, 307, 319,\n",
       "       331, 336, 338, 354, 369, 399, 421, 468, 475])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb_feats = np.where(skb.get_support())[0]\n",
    "skb_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** RFE **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression with L1 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "rfe1 =RFECV(estimator = lr_l1, step = 1, cv = 5, scoring = 'accuracy')\n",
    "pipe_rfe1 = Pipeline([\n",
    "    ('scaler', scaler),  \n",
    "    ('rfe1', rfe1) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('rfe1', RFECV(cv=5,\n",
       "   estimator=LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l1', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "   n_jobs=1, scoring='accuracy', step=1, verbose=0))])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rfe1.fit(X_train_bc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe1.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.69569657]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe1.estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe1_feat_coef_ind = np.argsort(np.abs(rfe1.estimator_.coef_))[0][::-1]\n",
    "rfe1_feat_coef_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69569657])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe1_feat_coef_val = np.sort(np.abs(rfe1.estimator_.coef_))[0][::-1]\n",
    "rfe1_feat_coef_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminates all the way down to one feature, this is odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "rfe2 = RFECV(estimator = RandomForestClassifier(), step = 1, cv = 5, scoring = 'accuracy')\n",
    "pipe_rfe2 = Pipeline([\n",
    "    ('scaler', scaler),  \n",
    "    ('rfe2', rfe2) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('rfe2', RFECV(cv=5,\n",
       "   estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_s...one,\n",
       "            verbose=0, warm_start=False),\n",
       "   n_jobs=1, scoring='accuracy', step=1, verbose=0))])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_rfe2.fit(X_train_bc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(208, 500)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe2.n_features_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe2.support_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rfe2.estimator_.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([286, 132, 151, 184,   8, 196,  19, 213, 112, 136, 171, 139, 176,\n",
       "        99, 121, 137, 248,   2, 109, 169, 279, 161,  57,   3,  50,  18,\n",
       "        76,  14, 100, 264,  81, 182,  10,  39, 277, 128,  38, 281,  25,\n",
       "       178,  13, 224, 225, 135, 245,  62, 229, 263, 192,  61, 113, 278,\n",
       "       133, 255, 102,  82,  44, 129, 222, 270, 188,  72, 211, 185,  98,\n",
       "       241, 104, 134,  83, 234,  68, 194, 190, 153, 254, 272, 235,  89,\n",
       "       114, 111, 166,  29, 285, 106, 198, 266, 274,  34, 103,  86,  48,\n",
       "        60, 127,   5,  46, 271, 267,  56,  11,  28, 157,  15, 242, 220,\n",
       "        84, 259,  69,  94, 172,  42, 258,  40, 174, 115, 268, 131, 143,\n",
       "        58,  90,  65, 204, 240, 150,  64,  21, 206, 280,  54, 149, 126,\n",
       "       275,  51, 260,  92, 122, 167,  20,   4, 168, 181,  26, 147, 256,\n",
       "       231, 193, 108, 140,  27, 214, 170, 107,  30, 142, 249, 243, 282,\n",
       "        49,  59, 237])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe2_feat_coef_ind = np.argsort(rfe2.estimator_.feature_importances_)[::-1]\n",
    "rfe2_feat_coef_ind[rfe2_feat_coef_val > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04485585,  0.02966088,  0.0238783 ,  0.02261411,  0.01775912,\n",
       "        0.01679263,  0.01448637,  0.014417  ,  0.01438966,  0.01396079,\n",
       "        0.01387631,  0.01384535,  0.01366317,  0.01335439,  0.01148754,\n",
       "        0.01145838,  0.01138588,  0.01135304,  0.01101868,  0.01099845,\n",
       "        0.01029088,  0.01008863,  0.01007899,  0.01007505,  0.00997555,\n",
       "        0.00988215,  0.00953312,  0.0095243 ,  0.00947595,  0.00945257,\n",
       "        0.00940293,  0.00919709,  0.00896023,  0.00886719,  0.00866805,\n",
       "        0.00826005,  0.0081726 ,  0.00786244,  0.00778583,  0.00762291,\n",
       "        0.00756881,  0.00750875,  0.00745133,  0.00743109,  0.0074044 ,\n",
       "        0.00733601,  0.00725482,  0.0072374 ,  0.00723374,  0.00715796,\n",
       "        0.0071055 ,  0.00709119,  0.00705008,  0.00701351,  0.00699961,\n",
       "        0.0069079 ,  0.00688454,  0.00686703,  0.00677324,  0.00672382,\n",
       "        0.00660922,  0.00660868,  0.00654034,  0.006409  ,  0.00639485,\n",
       "        0.00633628,  0.00633125,  0.00631558,  0.00627578,  0.00620043,\n",
       "        0.00613976,  0.0060666 ,  0.00597367,  0.00585635,  0.00584584,\n",
       "        0.00581356,  0.00574256,  0.00572394,  0.00563354,  0.00553428,\n",
       "        0.00528254,  0.00525455,  0.00521498,  0.00511001,  0.00504583,\n",
       "        0.00491145,  0.00482375,  0.00482055,  0.00481429,  0.00480838,\n",
       "        0.00479754,  0.00476098,  0.00454545,  0.00454521,  0.00441715,\n",
       "        0.00427354,  0.00396926,  0.00395641,  0.00391308,  0.00385758,\n",
       "        0.00382647,  0.00378685,  0.0035236 ,  0.00351119,  0.00350651,\n",
       "        0.00342162,  0.00340911,  0.00334137,  0.00331171,  0.00328691,\n",
       "        0.00323623,  0.00321643,  0.00315589,  0.00308091,  0.00283631,\n",
       "        0.00262626,  0.00257004,  0.00250602,  0.00240578,  0.00235232,\n",
       "        0.00198865,  0.00195549,  0.0019043 ,  0.00188406,  0.00187962,\n",
       "        0.00187887,  0.00185869,  0.00185685,  0.00185654,  0.00185457,\n",
       "        0.0018534 ,  0.00184889,  0.00181649,  0.00181013,  0.00179384,\n",
       "        0.00178069,  0.00177926,  0.00176298,  0.00171718,  0.00170095,\n",
       "        0.00167068,  0.00165585,  0.00164141,  0.00160494,  0.00154074,\n",
       "        0.00144565,  0.00133655,  0.00133655,  0.00130371,  0.00128968,\n",
       "        0.00128633,  0.00128217,  0.00098485,  0.00096591,  0.00092593,\n",
       "        0.00090278,  0.00088988,  0.00088293,  0.00045334])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfe2_feat_coef_val = np.sort(rfe2.estimator_.feature_importances_)[::-1]\n",
    "rfe2_feat_coef_val[rfe2_feat_coef_val > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rfe2_feat_coef_ind[rfe2_feat_coef_val > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding common features between three methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,  11,  33,  64, 105, 120, 121, 128, 165, 167, 175, 189, 191,\n",
       "       212, 224, 241, 260, 268, 271, 278, 283, 285, 289, 297, 307, 319,\n",
       "       331, 336, 338, 354, 369, 399, 421, 468, 475])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skb_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([241, 121, 369, 331,   3,  11, 399,  33, 307, 129, 260, 428, 105,\n",
       "       388, 285, 278, 319, 354, 191, 421, 283,   5, 268,   9, 224, 165,\n",
       "       491, 237, 485, 378, 289, 212, 446, 495, 431])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm2_feat_coef_ind[0:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([241, 475, 336,  64, 338, 331, 105, 399, 128,  33, 369, 285, 212,\n",
       "       121, 307,   3, 165, 191, 224,  11, 421, 189, 268, 271, 297, 175,\n",
       "       354, 260, 167, 283, 278, 120, 289, 319, 468])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfm1_feat_coef_ind[0:35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
